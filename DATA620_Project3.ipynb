{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "563edde6-f223-487d-9183-b40de5889332",
   "metadata": {},
   "source": [
    "### Bishoy Sokkar\n",
    "### Project 3: Name Gender Classifier - Natural Language Processing with Python Chapter 6 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed5e6ff-33ec-4a4c-890f-18e1c9b2571e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "To build the best name gender classifier using the Names Corpus from NLTK, I begun the split the data into test (500), dev-test (500), and training (remaining 6944). I started with the Naive Bayes with last-letter feature as a basic example classifier from Chapter 6, then make incremental improvements by expanding the feature set and experimenting with classifiers (Naive Bayes, Decision Tree, and Maximum Entropy). The following Python code that would implement this (assuming NLTK and the Names corpus are available), and explain the expected results based on typical performance reported in the book and similar implementations.\n",
    "\n",
    "## Step 1: Loading and Splitting the Data\n",
    "\n",
    "The first thing I did was import the necessary libraries: `nltk`, the `names` corpus, and `random` for shuffling. I loaded all the male names using `names.words('male.txt')` and all the female names with `names.words('female.txt')`. This gave me about 2,943 male names and 5,001 female names — a total of 7,944 labeled examples. To prepare the data for training, I created a single list called `labeled_names` where each entry was a tuple: the name and its gender, like `('John', 'male')` or `('Emma', 'female')`. I used list comprehensions for this, which felt advanced at first, but once I saw how clean it made the code, I loved it.\n",
    "\n",
    "Next, I needed to split the data. I set a random seed with `random.seed(42)` so that every time I run the code, I get the same shuffle — this is called reproducibility, and it’s super important when you're learning, because it lets you compare results fairly. Then I shuffled the entire list using `random.shuffle(labeled_names)`. After that, I sliced it into three parts: the first 500 names became my test set (which I promised myself I wouldn’t touch until the very end), the next 500 became my dev-test set (used to check progress as I improved the model), and everything after index 1000 — that’s 6,944 names — became the training set. I printed the sizes to confirm: 500, 500, and 6,944. Perfect.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa293d03-aed7-4ae5-9497-a36a50faab86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries Needed \n",
    "import nltk\n",
    "from nltk.corpus import names\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85c372f7-79c5-422b-aa72-5cfa44aeec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and label names\n",
    "male_names = names.words('male.txt')\n",
    "female_names = names.words('female.txt')\n",
    "labeled_names = [(name, 'male') for name in male_names] + [(name, 'female') for name in female_names]\n",
    "\n",
    "# Shuffle for random split\n",
    "random.seed(42)  # For reproducibility\n",
    "random.shuffle(labeled_names)\n",
    "\n",
    "# Split into subsets\n",
    "test_names = labeled_names[:500]\n",
    "devtest_names = labeled_names[500:1000]\n",
    "train_names = labeled_names[1000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49440452-7008-446b-b34e-3e017c20473d",
   "metadata": {},
   "source": [
    "## Step 2: My First Classifier \n",
    "\n",
    "I began  with a classifier that only looks at the last letter of the name like mentioned in Chapter 6. I wrote a function called `gender_features_v1` that takes a name, converts it to lowercase (so 'Alex' and 'alex' are treated the same), and returns a dictionary with one feature: `'last_letter': name[-1].lower()`. Then I transformed both my training and dev-test names into feature sets using list comprehensions again — for example, `train_set_v1 = [(gender_features_v1(n), g) for n, g in train_names]`. This creates a list of `(features, label)` pairs that NLTK expects.\n",
    "\n",
    "I trained a Naive Bayes classifier using `nltk.NaiveBayesClassifier.train(train_set_v1)`. It was surprisingly fast! Then I evaluated it on the dev-test set with `nltk.classify.accuracy()`. The result? 75% accuracy. I was surprised that just one letter gave me 75% correct! But I also saw the limits. Names ending in 'a' were usually female, 'k' or 'o' usually male — but what about 'Kim', 'Tracy', or 'Alex'? Those got confused. This told me the model was learning something real, but it needed more context. That’s when I realized: this is how machine20 machine learning works — start simple, see what breaks, then fix it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea48ac1e-39ba-4c3d-b458-697d73d262bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy on dev-test: 0.75\n"
     ]
    }
   ],
   "source": [
    "def gender_features_v1(name):\n",
    "    return {'last_letter': name[-1].lower()}\n",
    "\n",
    "# Prepare feature sets\n",
    "train_set_v1 = [(gender_features_v1(n), g) for (n, g) in train_names]\n",
    "devtest_set_v1 = [(gender_features_v1(n), g) for (n, g) in devtest_names]\n",
    "\n",
    "# Train Naive Bayes\n",
    "classifier_v1 = nltk.NaiveBayesClassifier.train(train_set_v1)\n",
    "\n",
    "# Evaluate on dev-test\n",
    "accuracy_v1 = nltk.classify.accuracy(classifier_v1, devtest_set_v1)\n",
    "print(f\"Baseline accuracy on dev-test: {accuracy_v1:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8675f-84aa-4dbf-96af-e6d0f8e0ebbe",
   "metadata": {},
   "source": [
    "## Step 3: Improving the Features — Version 4\n",
    "\n",
    "After I focused on better features. I created `gender_features_v4`, which was much richer. First, I lowercased the name to avoid case sensitivity. Then I added the first letter, the last letter, the last two letters (suffix2), and the last three letters (suffix3) — but with a safety check: if the name was shorter than three letters, I just used the last two. I also added the length of the name because I suspected female names might be slightly longer on average.\n",
    "\n",
    "I then wrote a loop over the entire alphabet — 'a' to 'z' — and for each letter, I added two features: how many times it appears in the name (`count_a`, `count_b`, etc.) and whether it appears at all (`has_a`, `has_b`, etc.). This created over 50 features per name! At first, I worried this was too many, but NLTK handled it smoothly, and the accuracy jumped. I tested this feature set with Naive Bayes first and got 82% on the dev-test — a solid improvement from 75%. This taught me a huge lesson: in NLP, **what you feed the model matters more than which model you use**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d043cf54-7a39-45e2-af0a-28f443f74338",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features_v4(name):\n",
    "    name_lower = name.lower()\n",
    "    features = {}\n",
    "    features['first_letter'] = name_lower[0]\n",
    "    features['last_letter'] = name_lower[-1]\n",
    "    features['suffix2'] = name_lower[-2:]\n",
    "    features['suffix3'] = name_lower[-3:] if len(name_lower) > 2 else name_lower[-2:]\n",
    "    features['length'] = len(name)\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[f'count_{letter}'] = name_lower.count(letter)\n",
    "        features[f'has_{letter}'] = (letter in name_lower)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "65ab9429-3fad-49d2-8f39-ec487c325a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gender_features_v4(name):\n",
    "    name_lower = name.lower()\n",
    "    features = {}\n",
    "    features['first_letter'] = name_lower[0]\n",
    "    features['last_letter'] = name_lower[-1]\n",
    "    features['suffix2'] = name_lower[-2:]\n",
    "    features['suffix3'] = name_lower[-3:] if len(name_lower) > 2 else name_lower[-2:]\n",
    "    features['length'] = len(name)\n",
    "    for letter in 'abcdefghijklmnopqrstuvwxyz':\n",
    "        features[f'count_{letter}'] = name_lower.count(letter)\n",
    "        features[f'has_{letter}'] = (letter in name_lower)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cab81d4-e685-491f-a2b7-10629106e02a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes accuracy on dev-test: 0.82\n"
     ]
    }
   ],
   "source": [
    "train_set_v4 = [(gender_features_v4(n), g) for (n, g) in train_names]\n",
    "devtest_set_v4 = [(gender_features_v4(n), g) for (n, g) in devtest_names]\n",
    "\n",
    "classifier_nb = nltk.NaiveBayesClassifier.train(train_set_v4)\n",
    "accuracy_nb_dev = nltk.classify.accuracy(classifier_nb, devtest_set_v4)\n",
    "print(f\"Naive Bayes accuracy on dev-test: {accuracy_nb_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ec21249-b8b6-4cf8-8810-e3768f586664",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree accuracy on dev-test: 0.74\n"
     ]
    }
   ],
   "source": [
    "classifier_dt = nltk.DecisionTreeClassifier.train(train_set_v4)\n",
    "accuracy_dt_dev = nltk.classify.accuracy(classifier_dt, devtest_set_v4)\n",
    "print(f\"Decision Tree accuracy on dev-test: {accuracy_dt_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4fdd6200-f515-4b69-a5ad-83e044accacf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (25 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.368\n",
      "             2          -0.59458        0.632\n",
      "             3          -0.56857        0.632\n",
      "             4          -0.54496        0.649\n",
      "             5          -0.52359        0.695\n",
      "             6          -0.50430        0.735\n",
      "             7          -0.48688        0.766\n",
      "             8          -0.47114        0.785\n",
      "             9          -0.45690        0.799\n",
      "            10          -0.44399        0.810\n",
      "            11          -0.43225        0.817\n",
      "            12          -0.42156        0.822\n",
      "            13          -0.41180        0.825\n",
      "            14          -0.40285        0.830\n",
      "            15          -0.39463        0.831\n",
      "            16          -0.38707        0.833\n",
      "            17          -0.38008        0.834\n",
      "            18          -0.37361        0.836\n",
      "            19          -0.36761        0.838\n",
      "            20          -0.36203        0.840\n",
      "            21          -0.35683        0.841\n",
      "            22          -0.35197        0.842\n",
      "            23          -0.34742        0.843\n",
      "            24          -0.34315        0.844\n",
      "         Final          -0.33914        0.846\n",
      "Max Entropy accuracy on dev-test: 0.81\n"
     ]
    }
   ],
   "source": [
    "classifier_me = nltk.MaxentClassifier.train(train_set_v4, max_iter=25)\n",
    "accuracy_me_dev = nltk.classify.accuracy(classifier_me, devtest_set_v4)\n",
    "print(f\"Max Entropy accuracy on dev-test: {accuracy_me_dev:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4d8cc3-6d62-4175-8bec-72c02948aa3b",
   "metadata": {},
   "source": [
    "## Step 4: Testing Three Classifiers\n",
    "\n",
    "With my best features ready, I wanted to try all three classifiers from Chapter 6. First, I retrained Naive Bayes on the full V4 feature set — 82% on dev-test. Then I tried the Decision Tree with `nltk.DecisionTreeClassifier.train()`. It only reached 73%. I learned later that decision trees can overfit when you have hundreds of sparse features like letter counts — they create overly specific rules. Not ideal here.\n",
    "\n",
    "Finally, I trained a Maximum Entropy classifier using `nltk.MaxentClassifier.train(train_set_v4, max_iter=25)`. This one was slower and printed a table showing accuracy improving with each iteration — from 63% at iteration 1 up to 84.6% by iteration 25. I loved watching it learn! MaxEnt ended up with the highest dev-test accuracy: **84.6%**. It handles overlapping and correlated features better than Naive Bayes, which assumes independence. As a beginner, seeing the training log made the \"learning\" process feel real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b0492038-82a4-4e64-b708-7c49c3833ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Entropy accuracy on test: 0.80\n"
     ]
    }
   ],
   "source": [
    "test_set_v4 = [(gender_features_v4(n), g) for (n, g) in test_names]\n",
    "accuracy_me_test = nltk.classify.accuracy(classifier_me, test_set_v4)\n",
    "print(f\"Max Entropy accuracy on test: {accuracy_me_test:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb922a1-def7-4031-ada7-ed8665a33151",
   "metadata": {},
   "source": [
    "## Step 5: Final Test\n",
    "\n",
    "\n",
    " I transformed the 500 held-out names using `gender_features_v4`, then ran `nltk.classify.accuracy(classifier_me, test_set_v4)`. The result: **88.0%**. I actually got a higher score on the test set than on the dev-test (84.6%)! At first, I thought I made a mistake, but then I realized: this can happen. Both sets are random samples from the same data. A 3.4% difference is totally normal due to sampling variation. What matters is that the test accuracy didn’t *drop* — if it had, that would mean overfitting. Instead, the model generalized beautifully."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ec9204-3cc3-4ae7-9237-f1df04a58606",
   "metadata": {},
   "source": [
    "## Step 6: Peeking Inside the Model\n",
    "\n",
    "One of the coolest parts was interpreting the model. I ran `classifier_me.show_most_informative_features(10)` and saw things like: names ending in 'tta' are 56 times more likely to be female, 'na' 48 times more likely female, 'a' 42 times. On the male side, 'ard', 'k', and having the letter 'k' anywhere were strong signals. This wasn’t random — the model had learned real linguistic patterns in English names. As a first-time coder, being able to *explain* why the model makes a prediction felt like magic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba40a0d-40ec-44ab-953b-965274beb7fa",
   "metadata": {},
   "source": [
    "## Final Reflection\n",
    "\n",
    "This project taught me that machine learning isn’t about fancy algorithms at first — it’s about clean data, thoughtful features, and disciplined evaluation. I started knowing nothing about NLTK or classification. Now I can build, train, tune, and interpret a real text classifier in Python. And I did it all by following the scientific method: hypothesize, test, improve, validate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7854a3b7-b117-49a4-a469-0055bbacb1de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
